{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/ML/ML Projects/calculate calories/playground-series-s5e5/train.csv\")\n",
    "test = pd.read_csv(\"/ML/ML Projects/calculate calories/playground-series-s5e5/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
       "       'Body_Temp', 'Calories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
       "       'Body_Temp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp',\n",
       "       'Calories', 'Sex_female', 'Sex_male'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.get_dummies(train, columns=[\"Sex\"], dtype=int)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['id', 'Calories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Sex_female  Sex_male\n",
       "0   36   189.0    82.0      26.0       101.0       41.0           0         1\n",
       "1   64   163.0    60.0       8.0        85.0       39.7           1         0\n",
       "2   51   161.0    64.0       7.0        84.0       39.8           1         0\n",
       "3   20   192.0    90.0      25.0       105.0       40.7           0         1\n",
       "4   38   166.0    61.0      25.0       102.0       40.6           1         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is Calories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "t_dep = tensor(train.Calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 36.0000, 189.0000,  82.0000,  ...,  41.0000,   0.0000,   1.0000],\n",
       "        [ 64.0000, 163.0000,  60.0000,  ...,  39.7000,   1.0000,   0.0000],\n",
       "        [ 51.0000, 161.0000,  64.0000,  ...,  39.8000,   1.0000,   0.0000],\n",
       "        ...,\n",
       "        [ 60.0000, 162.0000,  67.0000,  ...,  40.9000,   0.0000,   1.0000],\n",
       "        [ 45.0000, 182.0000,  91.0000,  ...,  40.3000,   0.0000,   1.0000],\n",
       "        [ 39.0000, 171.0000,  65.0000,  ...,  40.6000,   1.0000,   0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep = tensor(df.values, dtype=torch.float)\n",
    "t_indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750000, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we're going to manually do a single step of calculating predictions and loss for every row of our data.\n",
    "Our first model will be a simple linear model. We'll need a coefficient for each column in t_indep. We'll pick random numbers in the range (-0.5,0.5), and set our manual seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(442)\n",
    "\n",
    "n_coeff = t_indep.shape[1]\n",
    "coeffs = torch.rand(n_coeff)-0.5\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions will be calculated by multiplying each row by the coefficients, and adding them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.6644,  26.1915,  19.7539,  ..., -12.9033,   0.0000,   0.3136],\n",
       "        [-29.6256,  22.5884,  14.4540,  ..., -12.4942,   0.4876,   0.0000],\n",
       "        [-23.6079,  22.3112,  15.4177,  ..., -12.5257,   0.4876,   0.0000],\n",
       "        ...,\n",
       "        [-27.7740,  22.4498,  16.1404,  ..., -12.8719,   0.0000,   0.3136],\n",
       "        [-20.8305,  25.2214,  21.9220,  ..., -12.6830,   0.0000,   0.3136],\n",
       "        [-18.0531,  23.6970,  15.6586,  ..., -12.7775,   0.4876,   0.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep*coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make all the columns contain numbers from 0 to 1, by dividing each column by its max():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dep = t_dep.max()\n",
    "# t_dep = t_dep/val_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2109,  0.1180,  0.1497,  ..., -0.3109,  0.0000,  0.3136],\n",
       "        [-0.3750,  0.1017,  0.1095,  ..., -0.3011,  0.4876,  0.0000],\n",
       "        [-0.2988,  0.1005,  0.1168,  ..., -0.3018,  0.4876,  0.0000],\n",
       "        ...,\n",
       "        [-0.3516,  0.1011,  0.1223,  ..., -0.3102,  0.0000,  0.3136],\n",
       "        [-0.2637,  0.1136,  0.1661,  ..., -0.3056,  0.0000,  0.3136],\n",
       "        [-0.2285,  0.1067,  0.1186,  ..., -0.3079,  0.4876,  0.0000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep*coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create predictions from our linear model, by adding up the rows of the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (t_indep*coeffs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3444, -0.2123, -0.1213, -0.2326, -0.2262, -0.1212,  0.1255, -0.3555,\n",
       "        -0.2028, -0.4894])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88.4948, dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.abs(preds-t_dep).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating functions for pred and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\n",
    "def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing a gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to do a single \"epoch\" of gradient descent manually. The only thing we're going to automate is calculating gradients, because let's face it that's pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we'll need to call requires_grad_() on our coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we calculate our loss, PyTorch will keep track of all the steps, so we'll be able to get the gradients afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88.4948, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, t_indep, t_dep)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5243, -0.7869, -0.5693, -0.5140, -0.7460, -0.9647, -0.5010, -0.4990])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0486, -1.5739, -1.1386, -1.0281, -1.4919, -1.9295, -1.0019, -0.9981])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, t_indep, t_dep)\n",
    "loss.backward()\n",
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".grad values are have doubled. That's because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\n",
    "\n",
    "We can now do one gradient descent step, and check that our loss decreases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(87.4539, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss = calc_loss(coeffs, t_indep, t_dep)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    coeffs.sub_(coeffs.grad * 0.1)\n",
    "    coeffs.grad.zero_()\n",
    "    print(calc_loss(coeffs, t_indep, t_dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, any method that ends in _ changes its object in-place.\n",
    "a.sub_(b) subtracts b from a in-place.\n",
    "a.zero_() sets all elements of a tensor to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter\n",
    "trn_split,val_split=RandomSplitter(seed=42)(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 150000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "len(trn_indep),len(val_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    coeffs.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(coeffs, lr):\n",
    "    loss = calc_loss(coeffs, trn_indep, trn_dep)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"{loss:.3f}\", end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, lr=0.01):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs): one_epoch(coeffs, lr=lr)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.525; 86.791; 85.068; 83.399; 81.791; 80.250; 78.792; 77.408; 76.090; 74.844; 73.663; 72.537; 71.462; 70.437; 69.460; 68.528; 67.642; 66.796; 65.987; 65.215; 64.478; 63.772; 63.098; 62.452; 61.835; 61.243; 60.674; 60.127; 59.601; 59.097; 58.613; 58.148; 57.700; 57.270; 56.855; 56.455; 56.071; 55.702; 55.349; 55.012; 54.687; 54.376; 54.077; 53.789; 53.512; 53.246; 52.990; 52.742; 52.504; 52.274; 52.053; 51.839; 51.633; 51.434; 51.243; 51.059; 50.882; 50.712; 50.547; 50.388; 50.234; 50.085; 49.942; 49.803; 49.669; 49.540; 49.415; 49.294; 49.177; 49.064; 48.955; 48.849; 48.747; 48.648; 48.552; 48.459; 48.368; 48.280; 48.195; 48.112; 48.031; 47.953; 47.876; 47.802; 47.730; 47.660; 47.591; 47.524; 47.459; 47.395; 47.333; 47.273; 47.213; 47.156; 47.099; 47.044; 46.990; 46.937; 46.885; 46.834; 46.785; 46.736; 46.689; 46.642; 46.596; 46.551; 46.507; 46.463; 46.421; 46.379; 46.337; 46.297; 46.257; 46.217; 46.178; 46.139; 46.102; 46.064; 46.027; 45.990; 45.954; 45.918; 45.883; 45.848; 45.814; 45.779; 45.745; 45.712; 45.678; 45.645; 45.613; 45.580; 45.548; 45.516; 45.484; 45.453; 45.421; 45.390; 45.360; 45.329; 45.298; 45.268; 45.238; 45.208; 45.178; 45.149; 45.119; 45.090; 45.061; 45.032; 45.003; 44.974; 44.945; 44.917; 44.889; 44.860; 44.832; 44.804; 44.776; 44.748; 44.720; 44.692; 44.665; 44.637; 44.610; 44.583; 44.555; 44.528; 44.501; 44.474; 44.447; 44.420; 44.393; 44.366; 44.339; 44.312; 44.286; 44.259; 44.233; 44.206; 44.180; 44.153; 44.127; 44.100; 44.074; 44.048; 44.022; 43.995; 43.969; 43.943; 43.917; 43.891; 43.865; 43.839; 43.813; 43.787; 43.761; 43.735; 43.709; 43.683; 43.658; 43.632; 43.606; 43.580; 43.555; 43.529; 43.503; 43.477; 43.452; 43.426; 43.401; 43.375; 43.349; 43.324; 43.298; 43.273; 43.247; 43.222; 43.196; 43.171; 43.145; 43.120; 43.094; 43.069; 43.043; 43.018; 42.992; 42.967; 42.942; 42.916; 42.891; 42.865; 42.840; 42.815; 42.789; 42.764; 42.739; 42.713; 42.688; 42.663; 42.637; 42.612; 42.587; 42.562; 42.536; 42.511; 42.486; 42.460; 42.435; 42.410; 42.385; 42.360; 42.334; 42.309; 42.284; 42.259; 42.233; 42.208; 42.183; 42.158; 42.133; 42.107; 42.082; 42.057; 42.032; 42.007; 41.981; 41.956; 41.931; 41.906; 41.881; 41.856; 41.830; 41.805; 41.780; 41.755; 41.730; 41.705; 41.680; 41.654; 41.629; 41.604; 41.579; 41.554; 41.529; 41.504; 41.479; 41.453; 41.428; 41.403; 41.378; 41.353; 41.328; 41.303; 41.278; 41.253; 41.228; 41.203; 41.177; 41.152; 41.127; 41.102; 41.077; 41.052; 41.027; 41.002; 40.977; 40.952; 40.927; 40.902; 40.877; 40.852; 40.827; 40.802; 40.777; 40.752; 40.727; 40.702; 40.677; 40.651; 40.626; 40.601; 40.576; 40.551; 40.526; 40.501; 40.476; 40.451; 40.426; 40.401; 40.376; 40.351; 40.326; 40.301; 40.276; 40.251; 40.226; 40.201; 40.176; 40.152; 40.127; 40.102; 40.077; 40.052; 40.027; 40.002; 39.977; 39.952; 39.927; 39.902; 39.877; 39.852; 39.827; 39.802; 39.777; 39.752; 39.727; 39.702; 39.677; 39.653; 39.628; 39.603; 39.578; 39.553; 39.528; 39.503; 39.478; 39.453; 39.428; 39.403; 39.379; 39.354; 39.329; 39.304; 39.279; 39.254; 39.229; 39.204; 39.179; 39.155; 39.130; 39.105; 39.080; 39.055; 39.030; 39.005; 38.980; 38.956; 38.931; 38.906; 38.881; 38.856; 38.831; 38.806; 38.782; 38.757; 38.732; 38.707; 38.682; 38.657; 38.633; 38.608; 38.583; 38.558; 38.533; 38.508; 38.484; 38.459; 38.434; 38.409; 38.384; 38.359; 38.335; 38.310; 38.285; 38.260; 38.235; 38.211; 38.186; 38.161; 38.136; 38.111; 38.087; 38.062; 38.037; 38.012; 37.988; 37.963; 37.938; 37.913; 37.888; 37.864; 37.839; 37.814; 37.789; 37.765; 37.740; 37.715; 37.690; 37.666; 37.641; 37.616; 37.591; 37.567; 37.542; 37.517; 37.493; 37.468; 37.443; 37.418; 37.394; 37.369; 37.344; 37.319; 37.295; 37.270; 37.245; 37.221; 37.196; 37.171; 37.147; 37.122; 37.097; 37.072; 37.048; 37.023; 36.998; 36.974; 36.949; 36.924; 36.900; 36.875; 36.850; 36.826; 36.801; 36.776; 36.752; 36.727; 36.702; 36.678; 36.653; 36.628; 36.604; 36.579; 36.554; 36.530; 36.505; 36.481; 36.456; 36.431; 36.407; 36.382; 36.357; 36.333; 36.308; 36.284; 36.259; 36.234; 36.210; 36.185; 36.161; 36.136; 36.111; 36.087; 36.062; 36.038; 36.013; 35.988; 35.964; 35.939; 35.915; 35.890; 35.866; 35.841; 35.816; 35.792; 35.767; 35.743; 35.718; 35.694; 35.669; 35.644; 35.620; 35.595; 35.571; 35.546; 35.522; 35.497; 35.473; 35.448; 35.424; 35.399; 35.375; 35.350; 35.326; 35.301; 35.277; 35.252; 35.228; 35.203; 35.179; 35.154; 35.130; 35.105; 35.081; 35.056; 35.032; 35.007; 34.983; 34.958; 34.934; 34.909; 34.885; 34.860; 34.836; 34.811; 34.787; 34.762; 34.738; 34.713; 34.689; 34.665; 34.640; 34.616; 34.591; 34.567; 34.542; 34.518; 34.493; 34.469; 34.445; 34.420; 34.396; 34.371; 34.347; 34.322; 34.298; 34.274; 34.249; 34.225; 34.200; 34.176; 34.152; 34.127; 34.103; 34.079; 34.054; 34.030; 34.005; 33.981; 33.957; 33.932; 33.908; 33.884; 33.859; 33.835; 33.810; 33.786; 33.762; 33.737; 33.713; 33.689; 33.664; 33.640; 33.616; 33.591; 33.567; 33.543; 33.518; 33.494; 33.470; 33.446; 33.421; 33.397; 33.373; 33.348; 33.324; 33.300; 33.275; 33.251; 33.227; 33.203; 33.178; 33.154; 33.130; 33.105; 33.081; 33.057; 33.033; 33.008; 32.984; 32.960; 32.936; 32.911; 32.887; 32.863; 32.839; 32.814; 32.790; 32.766; 32.742; 32.717; 32.693; 32.669; 32.645; 32.621; 32.596; 32.572; 32.548; 32.524; 32.500; 32.475; 32.451; 32.427; 32.403; 32.379; 32.354; 32.330; 32.306; 32.282; 32.258; 32.234; 32.209; 32.185; 32.161; 32.137; 32.113; 32.089; 32.064; 32.040; 32.016; 31.992; 31.968; 31.944; 31.920; 31.895; 31.871; 31.847; 31.823; 31.799; 31.775; 31.751; 31.727; 31.703; 31.678; 31.654; 31.630; 31.606; 31.582; 31.558; 31.534; 31.510; 31.486; 31.462; 31.437; 31.413; 31.389; 31.365; 31.341; 31.317; 31.293; 31.269; 31.245; 31.221; 31.197; 31.173; 31.149; 31.125; 31.101; 31.077; 31.053; 31.029; 31.005; 30.981; 30.957; 30.933; 30.909; 30.885; 30.861; 30.837; 30.813; 30.789; 30.765; 30.741; 30.717; 30.693; 30.669; 30.645; 30.621; 30.597; 30.573; 30.549; 30.525; 30.501; 30.477; 30.453; 30.429; 30.405; 30.381; 30.357; 30.333; 30.309; 30.285; 30.262; 30.238; 30.214; 30.190; 30.166; 30.142; 30.118; 30.094; 30.070; 30.046; 30.023; 29.999; 29.975; 29.951; 29.927; 29.903; 29.879; 29.855; 29.832; 29.808; 29.784; 29.760; 29.736; 29.712; 29.688; 29.665; 29.641; 29.617; 29.593; 29.569; 29.546; 29.522; 29.498; 29.474; 29.450; 29.427; 29.403; 29.379; 29.355; 29.331; 29.308; 29.284; 29.260; 29.236; 29.213; 29.189; 29.165; 29.141; 29.117; 29.094; 29.070; 29.046; 29.023; 28.999; 28.975; 28.951; 28.928; 28.904; 28.880; 28.856; 28.833; 28.809; 28.785; 28.762; 28.738; 28.714; 28.691; 28.667; 28.643; 28.620; 28.596; 28.572; 28.549; 28.525; 28.501; 28.478; 28.454; 28.430; 28.407; 28.383; 28.360; 28.336; 28.312; 28.289; 28.265; 28.241; 28.218; 28.194; 28.171; 28.147; 28.123; 28.100; 28.076; 28.053; 28.029; 28.006; 27.982; 27.958; 27.935; 27.911; 27.888; 27.864; 27.841; 27.817; 27.794; 27.770; 27.747; 27.723; 27.699; 27.676; 27.652; 27.629; 27.605; 27.582; 27.558; 27.535; 27.511; 27.488; 27.465; 27.441; 27.418; 27.394; 27.371; 27.347; 27.324; 27.300; 27.277; 27.253; 27.230; 27.207; 27.183; 27.160; 27.136; 27.113; 27.090; 27.066; 27.043; 27.019; 26.996; 26.973; 26.949; 26.926; 26.903; 26.879; 26.856; 26.832; 26.809; 26.786; 26.762; 26.739; 26.716; 26.692; 26.669; 26.646; 26.623; 26.599; 26.576; 26.553; 26.529; 26.506; 26.483; 26.459; 26.436; 26.413; 26.390; 26.366; 26.343; 26.320; 26.297; 26.273; 26.250; 26.227; 26.204; 26.181; 26.157; 26.134; 26.111; 26.088; 26.064; 26.041; 26.018; 25.995; 25.972; 25.949; 25.925; 25.902; 25.879; 25.856; 25.833; 25.810; 25.786; 25.763; 25.740; 25.717; 25.694; 25.671; 25.648; 25.625; 25.602; 25.578; 25.555; 25.532; 25.509; 25.486; 25.463; 25.440; 25.417; 25.394; 25.371; 25.348; 25.325; 25.302; 25.279; 25.256; 25.232; 25.209; 25.186; 25.163; 25.140; 25.117; 25.094; 25.071; 25.048; 25.026; 25.003; 24.980; 24.957; 24.934; 24.911; 24.888; 24.865; 24.842; 24.819; 24.796; 24.773; 24.750; 24.727; 24.704; 24.682; 24.659; 24.636; 24.613; 24.590; 24.567; 24.544; 24.521; 24.499; 24.476; 24.453; 24.430; 24.407; 24.384; 24.362; 24.339; 24.316; 24.293; 24.270; 24.248; 24.225; 24.202; 24.179; 24.156; 24.134; 24.111; 24.088; 24.065; 24.043; 24.020; 23.997; 23.975; 23.952; 23.929; 23.906; 23.884; 23.861; 23.838; 23.816; 23.793; 23.770; 23.748; 23.725; 23.702; 23.680; 23.657; 23.634; 23.612; 23.589; 23.567; 23.544; 23.521; 23.499; 23.476; 23.454; 23.431; 23.408; 23.386; 23.363; 23.341; 23.318; 23.296; 23.273; 23.250; 23.228; 23.205; 23.183; 23.160; 23.138; 23.115; 23.093; 23.070; 23.048; 23.025; 23.003; 22.981; 22.958; 22.936; 22.913; 22.891; 22.868; 22.846; 22.824; 22.801; 22.779; 22.756; 22.734; 22.712; 22.689; 22.667; 22.644; 22.622; 22.600; 22.577; 22.555; 22.533; 22.510; 22.488; 22.466; 22.444; 22.421; 22.399; 22.377; 22.354; 22.332; 22.310; 22.288; 22.265; 22.243; 22.221; 22.199; 22.177; 22.154; 22.132; 22.110; 22.088; 22.066; 22.043; 22.021; 21.999; 21.977; 21.955; 21.933; 21.911; 21.888; 21.866; 21.844; 21.822; 21.800; 21.778; 21.756; 21.734; 21.712; 21.690; 21.668; 21.646; 21.624; 21.602; 21.580; 21.558; 21.536; 21.514; 21.492; 21.470; 21.448; 21.426; 21.404; 21.382; 21.360; 21.338; 21.316; 21.294; 21.272; 21.250; 21.228; 21.207; 21.185; 21.163; 21.141; 21.119; 21.097; 21.075; 21.054; 21.032; 21.010; 20.988; 20.966; 20.945; 20.923; 20.901; 20.879; 20.858; 20.836; 20.814; 20.792; 20.771; 20.749; 20.727; 20.705; 20.684; 20.662; 20.640; 20.619; 20.597; 20.575; 20.554; 20.532; 20.511; 20.489; 20.467; 20.446; 20.424; 20.403; 20.381; 20.359; 20.338; 20.316; 20.295; 20.273; 20.252; 20.230; 20.209; 20.187; 20.166; 20.144; 20.123; 20.101; 20.080; 20.059; 20.037; 20.016; 19.994; 19.973; 19.951; 19.930; 19.909; 19.887; 19.866; 19.845; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(1200, lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': tensor(5.1482),\n",
       " 'Height': tensor(-6.9203),\n",
       " 'Weight': tensor(-4.7393),\n",
       " 'Duration': tensor(132.3190),\n",
       " 'Heart_Rate': tensor(28.0554),\n",
       " 'Body_Temp': tensor(-0.2163),\n",
       " 'Sex_female': tensor(-2.4490),\n",
       " 'Sex_male': tensor(-4.9456)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_coeffs(): return dict(zip(df.columns, coeffs.requires_grad_(False)))\n",
    "show_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition is not, however, scored by absolute error (which is our loss function). It's scored by RMSLE. Let's see how accurate we were on the validation set. First, calculate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = calc_preds(coeffs, val_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 56.8126,  36.8158,  27.5813,  64.0966, 117.1311,  81.6798,  80.8388,\n",
       "         93.5057, 143.2396,  52.3924,  96.7526, 118.6191, 136.0366,  71.3817,\n",
       "         73.8211,  85.4227,  65.8850,  73.1081,  38.7274,  14.0739])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = preds\n",
    "results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(pred, val_dep):\n",
    "    rmsle = torch.tensor((torch.log(1+pred) - torch.log(1+val_dep)).mean())\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0955, dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(preds, val_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "Sex           0\n",
       "Age           0\n",
       "Height        0\n",
       "Weight        0\n",
       "Duration      0\n",
       "Heart_Rate    0\n",
       "Body_Temp     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.get_dummies(test_df, columns=[\"Sex\"], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_indep = tensor(test_df.values, dtype=torch.float)\n",
    "tst_indep = tst_indep/vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = calc_preds(coeffs, tst_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Calories'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test[['id','Calories']]\n",
    "sub_df.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,Calories\n",
      "750000,39.297047\n",
      "750001,97.17057\n",
      "750002,83.24481\n",
      "750003,103.56349\n",
      "750004,82.66986\n",
      "750005,36.76356\n",
      "750006,48.839397\n",
      "750007,15.873104\n",
      "750008,22.898008\n"
     ]
    }
   ],
   "source": [
    "!head sub.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we need predictions starting from 0, we can use Relu function for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_preds(coeffs, indeps): \n",
    "    tot = (indeps*coeffs).sum(axis=1)\n",
    "    return torch.relu(tot)  # ReLu\n",
    "def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    coeffs.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(coeffs, lr):\n",
    "    loss = calc_loss(coeffs, trn_indep, trn_dep)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): update_coeffs(coeffs, lr)\n",
    "    print(f\"{loss:.3f}\", end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, lr=0.01):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs): one_epoch(coeffs, lr=lr)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.309; 88.296; 88.077; 86.466; 84.748; 83.091; 81.493; 79.966; 78.523; 77.152; 75.849; 74.616; 73.447; 72.331; 71.265; 70.250; 69.282; 68.359; 67.481; 66.642; 65.841; 65.076; 64.345; 63.646; 62.977; 62.337; 61.725; 61.138; 60.573; 60.030; 59.509; 59.008; 58.528; 58.067; 57.623; 57.195; 56.783; 56.386; 56.005; 55.639; 55.290; 54.955; 54.633; 54.324; 54.027; 53.741; 53.467; 53.202; 52.948; 52.703; 52.466; 52.238; 52.018; 51.805; 51.601; 51.404; 51.214; 51.032; 50.856; 50.686; 50.523; 50.365; 50.212; 50.064; 49.922; 49.784; 49.651; 49.522; 49.398; 49.278; 49.162; 49.050; 48.941; 48.836; 48.735; 48.636; 48.541; 48.448; 48.358; 48.271; 48.186; 48.103; 48.023; 47.945; 47.869; 47.795; 47.723; 47.653; 47.585; 47.519; 47.454; 47.390; 47.329; 47.268; 47.209; 47.152; 47.095; 47.040; 46.986; 46.934; 46.882; 46.832; 46.782; 46.734; 46.687; 46.640; 46.594; 46.549; 46.505; 46.462; 46.420; 46.378; 46.337; 46.296; 46.256; 46.217; 46.178; 46.139; 46.101; 46.064; 46.027; 45.991; 45.954; 45.919; 45.884; 45.849; 45.814; 45.780; 45.746; 45.713; 45.679; 45.646; 45.614; 45.581; 45.549; 45.517; 45.485; 45.454; 45.423; 45.392; 45.361; 45.330; 45.300; 45.270; 45.240; 45.210; 45.180; 45.151; 45.121; 45.092; 45.063; 45.034; 45.005; 44.976; 44.947; 44.919; 44.891; 44.862; 44.834; 44.806; 44.778; 44.750; 44.722; 44.695; 44.667; 44.640; 44.612; 44.585; 44.558; 44.530; 44.503; 44.476; 44.449; 44.422; 44.395; 44.368; 44.342; 44.315; 44.288; 44.262; 44.235; 44.209; 44.182; 44.156; 44.129; 44.103; 44.077; 44.050; 44.024; 43.998; 43.972; 43.946; 43.920; 43.894; 43.868; 43.842; 43.816; 43.790; 43.764; 43.738; 43.712; 43.686; 43.660; 43.635; 43.609; 43.583; 43.557; 43.532; 43.506; 43.480; 43.455; 43.429; 43.403; 43.378; 43.352; 43.327; 43.301; 43.276; 43.250; 43.225; 43.199; 43.174; 43.148; 43.123; 43.097; 43.072; 43.046; 43.021; 42.995; 42.970; 42.945; 42.919; 42.894; 42.869; 42.843; 42.818; 42.792; 42.767; 42.742; 42.717; 42.691; 42.666; 42.641; 42.615; 42.590; 42.565; 42.539; 42.514; 42.489; 42.464; 42.438; 42.413; 42.388; 42.363; 42.337; 42.312; 42.287; 42.262; 42.237; 42.211; 42.186; 42.161; 42.136; 42.111; 42.085; 42.060; 42.035; 42.010; 41.985; 41.960; 41.934; 41.909; 41.884; 41.859; 41.834; 41.809; 41.783; 41.758; 41.733; 41.708; 41.683; 41.658; 41.633; 41.608; 41.582; 41.557; 41.532; 41.507; 41.482; 41.457; 41.432; 41.407; 41.382; 41.357; 41.331; 41.306; 41.281; 41.256; 41.231; 41.206; 41.181; 41.156; 41.131; 41.106; 41.081; 41.056; 41.031; 41.006; 40.980; 40.955; 40.930; 40.905; 40.880; 40.855; 40.830; 40.805; 40.780; 40.755; 40.730; 40.705; 40.680; 40.655; 40.630; 40.605; 40.580; 40.555; 40.530; 40.505; 40.480; 40.455; 40.430; 40.405; 40.380; 40.355; 40.330; 40.305; 40.280; 40.255; 40.230; 40.205; 40.180; 40.155; 40.130; 40.105; 40.080; 40.055; 40.030; 40.005; 39.980; 39.955; 39.931; 39.906; 39.881; 39.856; 39.831; 39.806; 39.781; 39.756; 39.731; 39.706; 39.681; 39.656; 39.631; 39.606; 39.582; 39.557; 39.532; 39.507; 39.482; 39.457; 39.432; 39.407; 39.382; 39.357; 39.333; 39.308; 39.283; 39.258; 39.233; 39.208; 39.183; 39.158; 39.133; 39.109; 39.084; 39.059; 39.034; 39.009; 38.984; 38.959; 38.935; 38.910; 38.885; 38.860; 38.835; 38.810; 38.785; 38.761; 38.736; 38.711; 38.686; 38.661; 38.636; 38.612; 38.587; 38.562; 38.537; 38.512; 38.487; 38.463; 38.438; 38.413; 38.388; 38.363; 38.339; 38.314; 38.289; 38.264; 38.239; 38.215; 38.190; 38.165; 38.140; 38.115; 38.091; 38.066; 38.041; 38.016; 37.991; 37.967; 37.942; 37.917; 37.892; 37.868; 37.843; 37.818; 37.793; 37.769; 37.744; 37.719; 37.694; 37.670; 37.645; 37.620; 37.595; 37.571; 37.546; 37.521; 37.496; 37.472; 37.447; 37.422; 37.398; 37.373; 37.348; 37.323; 37.299; 37.274; 37.249; 37.225; 37.200; 37.175; 37.151; 37.126; 37.101; 37.076; 37.052; 37.027; 37.002; 36.978; 36.953; 36.928; 36.904; 36.879; 36.854; 36.830; 36.805; 36.780; 36.756; 36.731; 36.706; 36.682; 36.657; 36.632; 36.608; 36.583; 36.559; 36.534; 36.509; 36.485; 36.460; 36.435; 36.411; 36.386; 36.362; 36.337; 36.312; 36.288; 36.263; 36.238; 36.214; 36.189; 36.165; 36.140; 36.115; 36.091; 36.066; 36.042; 36.017; 35.993; 35.968; 35.943; 35.919; 35.894; 35.870; 35.845; 35.821; 35.796; 35.771; 35.747; 35.722; 35.698; 35.673; 35.649; 35.624; 35.600; 35.575; 35.550; 35.526; 35.501; 35.477; 35.452; 35.428; 35.403; 35.379; 35.354; 35.330; 35.305; 35.281; 35.256; 35.232; 35.207; 35.183; 35.158; 35.134; 35.109; 35.085; 35.060; 35.036; 35.011; 34.987; 34.962; 34.938; 34.913; 34.889; 34.864; 34.840; 34.815; 34.791; 34.767; 34.742; 34.718; 34.693; 34.669; 34.644; 34.620; 34.595; 34.571; 34.547; 34.522; 34.498; 34.473; 34.449; 34.424; 34.400; 34.376; 34.351; 34.327; 34.302; 34.278; 34.253; 34.229; 34.205; 34.180; 34.156; 34.132; 34.107; 34.083; 34.058; 34.034; 34.010; 33.985; 33.961; 33.937; 33.912; 33.888; 33.863; 33.839; 33.815; 33.790; 33.766; 33.742; 33.717; 33.693; 33.669; 33.644; 33.620; 33.596; 33.571; 33.547; 33.523; 33.498; 33.474; 33.450; 33.426; 33.401; 33.377; 33.353; 33.328; 33.304; 33.280; 33.255; 33.231; 33.207; 33.183; 33.158; 33.134; 33.110; 33.086; 33.061; 33.037; 33.013; 32.988; 32.964; 32.940; 32.916; 32.891; 32.867; 32.843; 32.819; 32.794; 32.770; 32.746; 32.722; 32.698; 32.673; 32.649; 32.625; 32.601; 32.576; 32.552; 32.528; 32.504; 32.480; 32.456; 32.431; 32.407; 32.383; 32.359; 32.335; 32.310; 32.286; 32.262; 32.238; 32.214; 32.190; 32.165; 32.141; 32.117; 32.093; 32.069; 32.045; 32.021; 31.996; 31.972; 31.948; 31.924; 31.900; 31.876; 31.852; 31.827; 31.803; 31.779; 31.755; 31.731; 31.707; 31.683; 31.659; 31.635; 31.610; 31.586; 31.562; 31.538; 31.514; 31.490; 31.466; 31.442; 31.418; 31.394; 31.370; 31.346; 31.322; 31.297; 31.273; 31.249; 31.225; 31.201; 31.177; 31.153; 31.129; 31.105; 31.081; 31.057; 31.033; 31.009; 30.985; 30.961; 30.937; 30.913; 30.889; 30.865; 30.841; 30.817; 30.793; 30.769; 30.745; 30.721; 30.697; 30.673; 30.649; 30.625; 30.601; 30.577; 30.553; 30.529; 30.505; 30.481; 30.457; 30.433; 30.410; 30.386; 30.362; 30.338; 30.314; 30.290; 30.266; 30.242; 30.218; 30.194; 30.170; 30.146; 30.123; 30.099; 30.075; 30.051; 30.027; 30.003; 29.979; 29.955; 29.931; 29.908; 29.884; 29.860; 29.836; 29.812; 29.788; 29.764; 29.741; 29.717; 29.693; 29.669; 29.645; 29.621; 29.598; 29.574; 29.550; 29.526; 29.502; 29.479; 29.455; 29.431; 29.407; 29.383; 29.360; 29.336; 29.312; 29.288; 29.264; 29.241; 29.217; 29.193; 29.169; 29.146; 29.122; 29.098; 29.074; 29.051; 29.027; 29.003; 28.980; 28.956; 28.932; 28.908; 28.885; 28.861; 28.837; 28.814; 28.790; 28.766; 28.742; 28.719; 28.695; 28.671; 28.648; 28.624; 28.600; 28.577; 28.553; 28.529; 28.506; 28.482; 28.459; 28.435; 28.411; 28.388; 28.364; 28.340; 28.317; 28.293; 28.270; 28.246; 28.222; 28.199; 28.175; 28.152; 28.128; 28.104; 28.081; 28.057; 28.034; 28.010; 27.986; 27.963; 27.939; 27.916; 27.892; 27.869; 27.845; 27.822; 27.798; 27.775; 27.751; 27.727; 27.704; 27.680; 27.657; 27.633; 27.610; 27.586; 27.563; 27.539; 27.516; 27.493; 27.469; 27.446; 27.422; 27.399; 27.375; 27.352; 27.328; 27.305; 27.281; 27.258; 27.235; 27.211; 27.188; 27.164; 27.141; 27.117; 27.094; 27.071; 27.047; 27.024; 27.001; 26.977; 26.954; 26.930; 26.907; 26.884; 26.860; 26.837; 26.814; 26.790; 26.767; 26.744; 26.720; 26.697; 26.674; 26.650; 26.627; 26.604; 26.580; 26.557; 26.534; 26.511; 26.487; 26.464; 26.441; 26.417; 26.394; 26.371; 26.348; 26.324; 26.301; 26.278; 26.255; 26.231; 26.208; 26.185; 26.162; 26.139; 26.115; 26.092; 26.069; 26.046; 26.023; 25.999; 25.976; 25.953; 25.930; 25.907; 25.884; 25.860; 25.837; 25.814; 25.791; 25.768; 25.745; 25.722; 25.698; 25.675; 25.652; 25.629; 25.606; 25.583; 25.560; 25.537; 25.514; 25.491; 25.467; 25.444; 25.421; 25.398; 25.375; 25.352; 25.329; 25.306; 25.283; 25.260; 25.237; 25.214; 25.191; 25.168; 25.145; 25.122; 25.099; 25.076; 25.053; 25.030; 25.007; 24.984; 24.961; 24.938; 24.915; 24.892; 24.869; 24.846; 24.824; 24.801; 24.778; 24.755; 24.732; 24.709; 24.686; 24.663; 24.640; 24.617; 24.595; 24.572; 24.549; 24.526; 24.503; 24.480; 24.457; 24.435; 24.412; 24.389; 24.366; 24.343; 24.321; 24.298; 24.275; 24.252; 24.229; 24.207; 24.184; 24.161; 24.138; 24.116; 24.093; 24.070; 24.047; 24.025; 24.002; 23.979; 23.956; 23.934; 23.911; 23.888; 23.866; 23.843; 23.820; 23.797; 23.775; 23.752; 23.729; 23.707; 23.684; 23.662; 23.639; 23.616; 23.594; 23.571; 23.548; 23.526; 23.503; 23.481; 23.458; 23.435; 23.413; 23.390; 23.368; 23.345; 23.323; 23.300; 23.278; 23.255; 23.233; 23.210; 23.187; 23.165; 23.142; 23.120; 23.097; 23.075; 23.053; 23.030; 23.008; 22.985; 22.963; 22.940; 22.918; 22.895; 22.873; 22.850; 22.828; 22.806; 22.783; 22.761; 22.738; 22.716; 22.694; 22.671; 22.649; 22.627; 22.604; 22.582; 22.560; 22.537; 22.515; 22.493; 22.470; 22.448; 22.426; 22.404; 22.381; 22.359; 22.337; 22.314; 22.292; 22.270; 22.248; 22.226; 22.203; 22.181; 22.159; 22.137; 22.114; 22.092; 22.070; 22.048; 22.026; 22.004; 21.981; 21.959; 21.937; 21.915; 21.893; 21.871; 21.849; 21.827; 21.805; 21.783; 21.760; 21.738; 21.716; 21.694; 21.672; 21.650; 21.628; 21.606; 21.584; 21.562; 21.540; 21.518; 21.496; 21.474; 21.452; 21.430; 21.408; 21.386; 21.364; 21.343; 21.321; 21.299; 21.277; 21.255; 21.233; 21.211; 21.189; 21.167; 21.146; 21.124; 21.102; 21.080; 21.058; 21.036; 21.015; 20.993; 20.971; 20.949; 20.927; 20.906; 20.884; 20.862; 20.840; 20.819; 20.797; 20.775; 20.753; 20.732; 20.710; 20.688; 20.667; 20.645; 20.623; 20.602; 20.580; 20.558; 20.537; 20.515; 20.493; 20.472; 20.450; 20.429; 20.407; 20.386; 20.364; 20.342; 20.321; 20.299; 20.278; 20.256; 20.235; 20.213; 20.192; 20.170; 20.149; 20.127; 20.106; 20.084; 20.063; 20.042; 20.020; 19.999; 19.977; 19.956; 19.935; 19.913; 19.892; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(1200, lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': tensor(5.1262),\n",
       " 'Height': tensor(-6.8705),\n",
       " 'Weight': tensor(-4.7058),\n",
       " 'Duration': tensor(132.1092),\n",
       " 'Heart_Rate': tensor(28.0378),\n",
       " 'Body_Temp': tensor(-0.1705),\n",
       " 'Sex_female': tensor(-2.4075),\n",
       " 'Sex_male': tensor(-4.9245)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coeff are similar to previous predictions for same no of epochs and learning rate. Further, we will see how it performs for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = calc_preds(coeffs, val_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0962, dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(preds, val_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = calc_preds(coeffs, tst_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Calories'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test[['id','Calories']]\n",
    "sub_df.to_csv('sub_relu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,Calories\n",
      "750000,39.348866\n",
      "750001,97.14482\n",
      "750002,83.26062\n",
      "750003,103.54146\n",
      "750004,82.678665\n",
      "750005,36.837868\n",
      "750006,48.876278\n",
      "750007,15.959247\n",
      "750008,22.983482\n"
     ]
    }
   ],
   "source": [
    "!head sub_relu.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([56.8435, 36.8913, 27.6741,  ..., 36.3021, 56.0637, 47.2507])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_indep*coeffs).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([56.8435, 36.8913, 27.6741,  ..., 36.3021, 56.0637, 47.2507])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indep@coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is faster as matrix multiplication in pytorch is highly optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do matrix-matrix products, we need to turn coeffs into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(): return (torch.rand(n_coeff, 1)-0.5).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dep = trn_dep[:,None]\n",
    "val_dep = val_dep[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 1200\n",
    "def train_model(epochs = 100, lr = 0.1):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs):\n",
    "        preds = torch.relu(trn_indep@coeffs)\n",
    "        loss = torch.abs((preds) - trn_dep).mean()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            coeffs.sub_(coeffs.grad * lr)\n",
    "            coeffs.grad.zero_()\n",
    "        print(f\"{loss:.3f}\", end=\"; \")\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.309; 88.296; 88.077; 86.466; 84.748; 83.091; 81.493; 79.966; 78.523; 77.153; 75.849; 74.616; 73.447; 72.332; 71.265; 70.251; 69.282; 68.359; 67.482; 66.643; 65.841; 65.076; 64.345; 63.646; 62.977; 62.337; 61.725; 61.137; 60.573; 60.030; 59.508; 59.008; 58.528; 58.066; 57.622; 57.194; 56.782; 56.385; 56.004; 55.638; 55.289; 54.954; 54.632; 54.323; 54.026; 53.741; 53.466; 53.201; 52.947; 52.702; 52.465; 52.237; 52.017; 51.805; 51.600; 51.403; 51.214; 51.031; 50.856; 50.686; 50.523; 50.365; 50.212; 50.065; 49.922; 49.785; 49.652; 49.523; 49.399; 49.279; 49.163; 49.051; 48.942; 48.837; 48.736; 48.637; 48.542; 48.449; 48.359; 48.272; 48.187; 48.104; 48.024; 47.946; 47.870; 47.797; 47.725; 47.655; 47.587; 47.520; 47.455; 47.392; 47.330; 47.270; 47.211; 47.153; 47.097; 47.042; 46.988; 46.935; 46.884; 46.833; 46.784; 46.736; 46.688; 46.641; 46.596; 46.551; 46.507; 46.464; 46.421; 46.379; 46.338; 46.298; 46.258; 46.218; 46.179; 46.141; 46.103; 46.065; 46.029; 45.992; 45.956; 45.920; 45.885; 45.850; 45.816; 45.781; 45.748; 45.714; 45.681; 45.648; 45.615; 45.583; 45.551; 45.519; 45.487; 45.456; 45.424; 45.393; 45.362; 45.332; 45.301; 45.271; 45.241; 45.211; 45.182; 45.152; 45.123; 45.093; 45.064; 45.035; 45.006; 44.977; 44.949; 44.920; 44.892; 44.864; 44.835; 44.807; 44.779; 44.752; 44.724; 44.696; 44.669; 44.641; 44.614; 44.586; 44.559; 44.532; 44.505; 44.477; 44.450; 44.424; 44.397; 44.370; 44.343; 44.316; 44.290; 44.263; 44.236; 44.210; 44.183; 44.157; 44.131; 44.104; 44.078; 44.052; 44.026; 43.999; 43.973; 43.947; 43.921; 43.895; 43.869; 43.843; 43.817; 43.791; 43.765; 43.739; 43.713; 43.687; 43.662; 43.636; 43.610; 43.584; 43.559; 43.533; 43.507; 43.482; 43.456; 43.430; 43.405; 43.379; 43.353; 43.328; 43.302; 43.277; 43.251; 43.226; 43.200; 43.175; 43.149; 43.124; 43.098; 43.073; 43.047; 43.022; 42.997; 42.971; 42.946; 42.920; 42.895; 42.870; 42.844; 42.819; 42.794; 42.768; 42.743; 42.718; 42.692; 42.667; 42.642; 42.616; 42.591; 42.566; 42.541; 42.515; 42.490; 42.465; 42.440; 42.414; 42.389; 42.364; 42.339; 42.313; 42.288; 42.263; 42.238; 42.212; 42.187; 42.162; 42.137; 42.112; 42.086; 42.061; 42.036; 42.011; 41.986; 41.961; 41.935; 41.910; 41.885; 41.860; 41.835; 41.810; 41.784; 41.759; 41.734; 41.709; 41.684; 41.659; 41.634; 41.609; 41.583; 41.558; 41.533; 41.508; 41.483; 41.458; 41.433; 41.408; 41.383; 41.357; 41.332; 41.307; 41.282; 41.257; 41.232; 41.207; 41.182; 41.157; 41.132; 41.107; 41.082; 41.056; 41.031; 41.006; 40.981; 40.956; 40.931; 40.906; 40.881; 40.856; 40.831; 40.806; 40.781; 40.756; 40.731; 40.706; 40.681; 40.656; 40.631; 40.606; 40.581; 40.556; 40.531; 40.506; 40.481; 40.456; 40.431; 40.406; 40.381; 40.356; 40.331; 40.306; 40.281; 40.256; 40.231; 40.206; 40.181; 40.156; 40.131; 40.106; 40.081; 40.056; 40.031; 40.006; 39.981; 39.956; 39.931; 39.906; 39.881; 39.856; 39.831; 39.807; 39.782; 39.757; 39.732; 39.707; 39.682; 39.657; 39.632; 39.607; 39.582; 39.557; 39.532; 39.507; 39.483; 39.458; 39.433; 39.408; 39.383; 39.358; 39.333; 39.308; 39.283; 39.258; 39.234; 39.209; 39.184; 39.159; 39.134; 39.109; 39.084; 39.059; 39.035; 39.010; 38.985; 38.960; 38.935; 38.910; 38.885; 38.861; 38.836; 38.811; 38.786; 38.761; 38.736; 38.711; 38.687; 38.662; 38.637; 38.612; 38.587; 38.562; 38.538; 38.513; 38.488; 38.463; 38.438; 38.414; 38.389; 38.364; 38.339; 38.314; 38.289; 38.265; 38.240; 38.215; 38.190; 38.165; 38.141; 38.116; 38.091; 38.066; 38.042; 38.017; 37.992; 37.967; 37.942; 37.918; 37.893; 37.868; 37.843; 37.819; 37.794; 37.769; 37.744; 37.720; 37.695; 37.670; 37.645; 37.621; 37.596; 37.571; 37.546; 37.522; 37.497; 37.472; 37.447; 37.423; 37.398; 37.373; 37.349; 37.324; 37.299; 37.274; 37.250; 37.225; 37.200; 37.176; 37.151; 37.126; 37.102; 37.077; 37.052; 37.027; 37.003; 36.978; 36.953; 36.929; 36.904; 36.879; 36.855; 36.830; 36.805; 36.781; 36.756; 36.731; 36.707; 36.682; 36.657; 36.633; 36.608; 36.584; 36.559; 36.534; 36.510; 36.485; 36.460; 36.436; 36.411; 36.386; 36.362; 36.337; 36.313; 36.288; 36.263; 36.239; 36.214; 36.190; 36.165; 36.140; 36.116; 36.091; 36.067; 36.042; 36.017; 35.993; 35.968; 35.944; 35.919; 35.895; 35.870; 35.845; 35.821; 35.796; 35.772; 35.747; 35.723; 35.698; 35.673; 35.649; 35.624; 35.600; 35.575; 35.551; 35.526; 35.502; 35.477; 35.453; 35.428; 35.404; 35.379; 35.354; 35.330; 35.305; 35.281; 35.256; 35.232; 35.207; 35.183; 35.158; 35.134; 35.109; 35.085; 35.060; 35.036; 35.011; 34.987; 34.962; 34.938; 34.914; 34.889; 34.865; 34.840; 34.816; 34.791; 34.767; 34.742; 34.718; 34.693; 34.669; 34.644; 34.620; 34.596; 34.571; 34.547; 34.522; 34.498; 34.473; 34.449; 34.425; 34.400; 34.376; 34.351; 34.327; 34.302; 34.278; 34.254; 34.229; 34.205; 34.180; 34.156; 34.132; 34.107; 34.083; 34.058; 34.034; 34.010; 33.985; 33.961; 33.937; 33.912; 33.888; 33.864; 33.839; 33.815; 33.790; 33.766; 33.742; 33.717; 33.693; 33.669; 33.644; 33.620; 33.596; 33.571; 33.547; 33.523; 33.498; 33.474; 33.450; 33.426; 33.401; 33.377; 33.353; 33.328; 33.304; 33.280; 33.255; 33.231; 33.207; 33.183; 33.158; 33.134; 33.110; 33.086; 33.061; 33.037; 33.013; 32.988; 32.964; 32.940; 32.916; 32.891; 32.867; 32.843; 32.819; 32.794; 32.770; 32.746; 32.722; 32.698; 32.673; 32.649; 32.625; 32.601; 32.576; 32.552; 32.528; 32.504; 32.480; 32.455; 32.431; 32.407; 32.383; 32.359; 32.335; 32.310; 32.286; 32.262; 32.238; 32.214; 32.190; 32.165; 32.141; 32.117; 32.093; 32.069; 32.045; 32.020; 31.996; 31.972; 31.948; 31.924; 31.900; 31.876; 31.852; 31.827; 31.803; 31.779; 31.755; 31.731; 31.707; 31.683; 31.659; 31.634; 31.610; 31.586; 31.562; 31.538; 31.514; 31.490; 31.466; 31.442; 31.418; 31.394; 31.370; 31.345; 31.321; 31.297; 31.273; 31.249; 31.225; 31.201; 31.177; 31.153; 31.129; 31.105; 31.081; 31.057; 31.033; 31.009; 30.985; 30.961; 30.937; 30.913; 30.889; 30.865; 30.841; 30.817; 30.793; 30.769; 30.745; 30.721; 30.697; 30.673; 30.649; 30.625; 30.601; 30.577; 30.553; 30.529; 30.505; 30.481; 30.457; 30.433; 30.409; 30.385; 30.361; 30.338; 30.314; 30.290; 30.266; 30.242; 30.218; 30.194; 30.170; 30.146; 30.122; 30.098; 30.074; 30.051; 30.027; 30.003; 29.979; 29.955; 29.931; 29.907; 29.883; 29.860; 29.836; 29.812; 29.788; 29.764; 29.740; 29.717; 29.693; 29.669; 29.645; 29.621; 29.597; 29.574; 29.550; 29.526; 29.502; 29.478; 29.454; 29.431; 29.407; 29.383; 29.359; 29.336; 29.312; 29.288; 29.264; 29.240; 29.217; 29.193; 29.169; 29.145; 29.122; 29.098; 29.074; 29.050; 29.027; 29.003; 28.979; 28.955; 28.932; 28.908; 28.884; 28.861; 28.837; 28.813; 28.790; 28.766; 28.742; 28.718; 28.695; 28.671; 28.647; 28.624; 28.600; 28.576; 28.553; 28.529; 28.505; 28.482; 28.458; 28.435; 28.411; 28.387; 28.364; 28.340; 28.316; 28.293; 28.269; 28.246; 28.222; 28.198; 28.175; 28.151; 28.128; 28.104; 28.080; 28.057; 28.033; 28.010; 27.986; 27.963; 27.939; 27.915; 27.892; 27.868; 27.845; 27.821; 27.798; 27.774; 27.751; 27.727; 27.704; 27.680; 27.657; 27.633; 27.610; 27.586; 27.563; 27.539; 27.516; 27.492; 27.469; 27.445; 27.422; 27.398; 27.375; 27.351; 27.328; 27.304; 27.281; 27.258; 27.234; 27.211; 27.187; 27.164; 27.140; 27.117; 27.094; 27.070; 27.047; 27.023; 27.000; 26.977; 26.953; 26.930; 26.907; 26.883; 26.860; 26.837; 26.813; 26.790; 26.767; 26.743; 26.720; 26.697; 26.673; 26.650; 26.627; 26.603; 26.580; 26.557; 26.533; 26.510; 26.487; 26.464; 26.440; 26.417; 26.394; 26.370; 26.347; 26.324; 26.301; 26.277; 26.254; 26.231; 26.208; 26.185; 26.161; 26.138; 26.115; 26.092; 26.069; 26.045; 26.022; 25.999; 25.976; 25.953; 25.929; 25.906; 25.883; 25.860; 25.837; 25.814; 25.790; 25.767; 25.744; 25.721; 25.698; 25.675; 25.652; 25.629; 25.606; 25.582; 25.559; 25.536; 25.513; 25.490; 25.467; 25.444; 25.421; 25.398; 25.375; 25.352; 25.329; 25.306; 25.283; 25.260; 25.236; 25.213; 25.190; 25.167; 25.144; 25.121; 25.098; 25.075; 25.052; 25.030; 25.007; 24.984; 24.961; 24.938; 24.915; 24.892; 24.869; 24.846; 24.823; 24.800; 24.777; 24.754; 24.731; 24.708; 24.686; 24.663; 24.640; 24.617; 24.594; 24.571; 24.548; 24.525; 24.503; 24.480; 24.457; 24.434; 24.411; 24.388; 24.366; 24.343; 24.320; 24.297; 24.274; 24.252; 24.229; 24.206; 24.183; 24.160; 24.138; 24.115; 24.092; 24.069; 24.047; 24.024; 24.001; 23.978; 23.956; 23.933; 23.910; 23.888; 23.865; 23.842; 23.820; 23.797; 23.774; 23.752; 23.729; 23.706; 23.684; 23.661; 23.638; 23.616; 23.593; 23.570; 23.548; 23.525; 23.503; 23.480; 23.457; 23.435; 23.412; 23.390; 23.367; 23.345; 23.322; 23.300; 23.277; 23.254; 23.232; 23.209; 23.187; 23.164; 23.142; 23.119; 23.097; 23.074; 23.052; 23.029; 23.007; 22.985; 22.962; 22.940; 22.917; 22.895; 22.872; 22.850; 22.827; 22.805; 22.783; 22.760; 22.738; 22.716; 22.693; 22.671; 22.648; 22.626; 22.604; 22.581; 22.559; 22.537; 22.514; 22.492; 22.470; 22.448; 22.425; 22.403; 22.381; 22.358; 22.336; 22.314; 22.292; 22.269; 22.247; 22.225; 22.203; 22.180; 22.158; 22.136; 22.114; 22.092; 22.070; 22.047; 22.025; 22.003; 21.981; 21.959; 21.937; 21.914; 21.892; 21.870; 21.848; 21.826; 21.804; 21.782; 21.760; 21.738; 21.716; 21.694; 21.672; 21.650; 21.628; 21.606; 21.584; 21.562; 21.540; 21.518; 21.496; 21.474; 21.452; 21.430; 21.408; 21.386; 21.364; 21.342; 21.320; 21.298; 21.276; 21.254; 21.232; 21.210; 21.189; 21.167; 21.145; 21.123; 21.101; 21.079; 21.058; 21.036; 21.014; 20.992; 20.970; 20.949; 20.927; 20.905; 20.883; 20.861; 20.840; 20.818; 20.796; 20.775; 20.753; 20.731; 20.709; 20.688; 20.666; 20.644; 20.623; 20.601; 20.579; 20.558; 20.536; 20.514; 20.493; 20.471; 20.450; 20.428; 20.406; 20.385; 20.363; 20.342; 20.320; 20.299; 20.277; 20.256; 20.234; 20.213; 20.191; 20.170; 20.148; 20.127; 20.105; 20.084; 20.062; 20.041; 20.020; 19.998; 19.977; 19.955; 19.934; 19.913; 19.891; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(1200, lr = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss observed during using matrix operation and ReLu function is almost same as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(pred, val_dep):\n",
    "    pred = pred.reshape(-1,1)\n",
    "    rmsle = torch.tensor((torch.log(1+pred) - torch.log(1+val_dep)).mean())\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0962, dtype=torch.float64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(preds, val_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSLE is same for validation set. Also, when using ones instead of initializing using random values, we get the same loss."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
