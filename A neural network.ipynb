{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/ML/ML Projects/calculate calories/playground-series-s5e5/train.csv\")\n",
    "test = pd.read_csv(\"/ML/ML Projects/calculate calories/playground-series-s5e5/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
       "       'Body_Temp', 'Calories'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
       "       'Body_Temp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp',\n",
       "       'Calories', 'Sex_female', 'Sex_male'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.get_dummies(train, columns=[\"Sex\"], dtype=int)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['id', 'Calories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Sex_female  Sex_male\n",
       "0   36   189.0    82.0      26.0       101.0       41.0           0         1\n",
       "1   64   163.0    60.0       8.0        85.0       39.7           1         0\n",
       "2   51   161.0    64.0       7.0        84.0       39.8           1         0\n",
       "3   20   192.0    90.0      25.0       105.0       40.7           0         1\n",
       "4   38   166.0    61.0      25.0       102.0       40.6           1         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is Calories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "t_dep = tensor(train.Calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 36.0000, 189.0000,  82.0000,  ...,  41.0000,   0.0000,   1.0000],\n",
       "        [ 64.0000, 163.0000,  60.0000,  ...,  39.7000,   1.0000,   0.0000],\n",
       "        [ 51.0000, 161.0000,  64.0000,  ...,  39.8000,   1.0000,   0.0000],\n",
       "        ...,\n",
       "        [ 60.0000, 162.0000,  67.0000,  ...,  40.9000,   0.0000,   1.0000],\n",
       "        [ 45.0000, 182.0000,  91.0000,  ...,  40.3000,   0.0000,   1.0000],\n",
       "        [ 39.0000, 171.0000,  65.0000,  ...,  40.6000,   1.0000,   0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep = tensor(df.values, dtype=torch.float)\n",
    "t_indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750000, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_indep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals,indices = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.transforms import RandomSplitter\n",
    "trn_split,val_split=RandomSplitter(seed=42)(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 150000, torch.Size([600000]), torch.Size([150000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\n",
    "trn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\n",
    "len(trn_indep),len(val_indep), trn_dep.shape, val_dep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coeff = t_indep.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([600000, 1]), torch.Size([150000, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_dep = trn_dep[:,None]\n",
    "val_dep = val_dep[:,None]\n",
    "trn_dep.shape, val_dep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([600000, 8]), torch.Size([600000, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_indep.shape, trn_dep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like -- a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We'll divide these coefficients by n_hidden so that when we sum them up in the next layer we'll end up with similar magnitude numbers to what we started with.\n",
    "\n",
    "Then our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs(n_hidden=40):\n",
    "    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n",
    "    layer2 = torch.rand(n_hidden, 1)-0.3\n",
    "    const = torch.rand(1)[0]\n",
    "    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that's our non-linearity), and the second is passed without any activation (linear activation) as this allows unbounded real outputs for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def calc_preds(coeffs, indeps):\n",
    "    l1,l2,const = coeffs\n",
    "    res = F.leaky_relu(indeps @ l1, negative_slope=0.01)\n",
    "    res = F.relu(res@l2 + const)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have more than one set of coefficients, we need to add a loop to update each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    for layer in coeffs:\n",
    "        layer.sub_(layer.grad * lr)\n",
    "        layer.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs = 12, lr = 0.1):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs):\n",
    "        res = calc_preds(coeffs, trn_indep)\n",
    "        loss = torch.abs((res) - trn_dep).mean()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            update_coeffs(coeffs, lr)\n",
    "        print(f\"{loss:.3f}\", end=\"; \")\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.238; 88.113; 87.919; 87.719; 87.514; 87.294; 87.044; 86.787; 86.519; 86.231; 85.921; 85.594; 85.246; 84.869; 84.471; 84.039; 83.583; 83.102; 82.589; 82.043; 81.461; 80.839; 80.161; 79.452; 78.707; 77.906; 77.059; 76.178; 75.267; 74.328; 73.359; 72.361; 71.335; 70.288; 69.223; 68.146; 67.062; 65.972; 64.886; 63.809; 62.744; 61.699; 60.673; 59.669; 58.695; 57.754; 56.847; 55.978; 55.158; 54.389; 53.669; 52.997; 52.370; 51.789; 51.255; 50.766; 50.318; 49.910; 49.541; 49.208; 48.906; 48.633; 48.385; 48.161; 47.957; 47.773; 47.605; 47.452; 47.313; 47.185; 47.066; 46.955; 46.851; 46.753; 46.661; 46.572; 46.488; 46.406; 46.328; 46.251; 46.177; 46.105; 46.034; 45.965; 45.896; 45.829; 45.762; 45.696; 45.630; 45.565; 45.500; 45.435; 45.371; 45.306; 45.242; 45.178; 45.113; 45.049; 44.984; 44.920; 44.855; 44.790; 44.725; 44.660; 44.594; 44.528; 44.462; 44.396; 44.330; 44.263; 44.196; 44.129; 44.061; 43.993; 43.925; 43.857; 43.788; 43.719; 43.650; 43.581; 43.511; 43.440; 43.370; 43.299; 43.227; 43.156; 43.084; 43.011; 42.938; 42.865; 42.792; 42.718; 42.644; 42.569; 42.494; 42.418; 42.342; 42.266; 42.189; 42.111; 42.033; 41.955; 41.876; 41.797; 41.718; 41.637; 41.557; 41.476; 41.394; 41.312; 41.229; 41.146; 41.062; 40.977; 40.892; 40.807; 40.721; 40.634; 40.547; 40.459; 40.370; 40.281; 40.191; 40.101; 40.010; 39.918; 39.826; 39.733; 39.639; 39.545; 39.450; 39.354; 39.257; 39.160; 39.062; 38.963; 38.863; 38.763; 38.662; 38.560; 38.457; 38.354; 38.249; 38.144; 38.038; 37.931; 37.823; 37.715; 37.605; 37.494; 37.383; 37.271; 37.157; 37.043; 36.928; 36.811; 36.694; 36.576; 36.457; 36.336; 36.215; 36.092; 35.969; 35.844; 35.718; 35.591; 35.463; 35.334; 35.204; 35.072; 34.939; 34.805; 34.670; 34.534; 34.396; 34.257; 34.117; 33.975; 33.833; 33.688; 33.543; 33.396; 33.248; 33.098; 32.947; 32.795; 32.641; 32.485; 32.329; 32.170; 32.011; 31.849; 31.686; 31.522; 31.356; 31.189; 31.020; 30.849; 30.677; 30.503; 30.327; 30.150; 29.971; 29.791; 29.608; 29.424; 29.239; 29.051; 28.862; 28.671; 28.478; 28.284; 28.087; 27.889; 27.689; 27.487; 27.283; 27.078; 26.870; 26.661; 26.450; 26.237; 26.022; 25.805; 25.587; 25.366; 25.144; 24.920; 24.694; 24.466; 24.237; 24.005; 23.772; 23.537; 23.301; 23.063; 22.822; 22.581; 22.337; 22.092; 21.845; 21.597; 21.347; 21.095; 20.842; 20.588; 20.332; 20.075; 19.817; 19.557; 19.296; 19.035; 18.772; 18.508; 18.244; 17.979; 17.714; 17.449; 17.183; 16.918; 16.654; 16.391; 16.130; 15.874; 15.623; 15.377; 15.138; 14.904; 14.678; 14.461; 14.252; 14.053; 13.864; 13.685; 13.516; 13.358; 13.209; 13.069; 12.936; 12.812; 12.693; 12.582; 12.476; 12.375; 12.279; 12.188; 12.101; 12.018; 11.938; 11.862; 11.788; 11.718; 11.650; 11.584; 11.520; 11.459; 11.399; 11.341; 11.285; 11.230; 11.177; 11.124; 11.073; 11.023; 10.973; 10.925; 10.877; 10.830; 10.784; 10.739; 10.694; 10.649; 10.605; 10.562; 10.519; 10.477; 10.435; 10.394; 10.353; 10.313; 10.273; 10.234; 10.196; 10.159; 10.122; 10.085; 10.050; 10.015; 9.980; 9.947; 9.913; 9.881; 9.849; 9.817; 9.786; 9.756; 9.726; 9.696; 9.667; 9.639; 9.611; 9.584; 9.557; 9.530; 9.504; 9.478; 9.453; 9.428; 9.404; 9.381; 9.357; 9.334; 9.312; 9.289; 9.267; 9.246; 9.225; 9.204; 9.183; 9.163; 9.143; 9.124; 9.104; 9.085; 9.067; 9.048; 9.030; 9.012; 8.995; 8.978; 8.961; 8.944; 8.928; 8.912; 8.896; 8.880; 8.865; 8.850; 8.835; 8.821; 8.806; 8.792; 8.778; 8.764; 8.751; 8.737; 8.724; 8.711; 8.698; 8.685; 8.673; 8.660; 8.648; 8.636; 8.624; 8.612; 8.600; 8.589; 8.578; 8.566; 8.555; 8.544; 8.533; 8.523; 8.512; 8.502; 8.491; 8.481; 8.471; 8.461; 8.451; 8.441; 8.432; 8.422; 8.412; 8.403; 8.394; 8.385; 8.376; 8.367; 8.358; 8.349; 8.340; 8.331; 8.323; 8.314; 8.306; 8.298; 8.289; 8.281; 8.273; 8.265; 8.257; 8.250; 8.242; 8.234; 8.227; 8.219; 8.212; 8.204; 8.197; 8.189; 8.182; 8.175; 8.168; 8.161; 8.154; 8.147; 8.140; 8.133; 8.127; 8.120; 8.113; 8.107; 8.100; 8.094; 8.087; 8.081; 8.075; 8.068; 8.062; 8.056; 8.050; 8.044; 8.038; 8.032; 8.026; 8.020; 8.014; 8.008; 8.002; 7.996; 7.991; 7.985; 7.979; 7.974; 7.968; 7.963; 7.957; 7.952; 7.946; 7.941; 7.936; 7.930; 7.925; 7.920; 7.915; 7.909; 7.904; 7.899; 7.894; 7.889; 7.884; 7.879; 7.874; 7.869; 7.864; 7.859; 7.855; 7.850; 7.845; 7.840; 7.835; 7.831; 7.826; 7.821; 7.817; 7.812; 7.808; 7.803; 7.799; 7.794; 7.790; 7.785; 7.781; 7.776; 7.772; 7.768; 7.763; 7.759; 7.755; 7.750; 7.746; 7.742; 7.738; 7.733; 7.729; 7.725; 7.721; 7.717; 7.713; 7.708; 7.704; 7.700; 7.696; 7.692; 7.688; 7.684; 7.680; 7.676; 7.672; 7.668; 7.664; 7.660; 7.657; 7.653; 7.649; 7.645; 7.641; 7.637; 7.633; 7.630; 7.626; 7.622; 7.618; 7.615; 7.611; 7.607; 7.603; 7.600; 7.596; 7.592; 7.589; 7.585; 7.581; 7.578; 7.574; 7.571; 7.567; 7.563; 7.560; 7.556; 7.553; 7.549; 7.546; 7.542; 7.539; 7.535; 7.532; 7.528; 7.525; 7.521; 7.518; 7.514; 7.511; 7.508; 7.504; 7.501; 7.497; 7.494; 7.491; 7.487; 7.484; 7.481; 7.477; 7.474; 7.471; 7.467; 7.464; 7.461; 7.458; 7.454; 7.451; 7.448; 7.445; 7.441; 7.438; 7.435; 7.432; 7.428; 7.425; 7.422; 7.419; 7.416; 7.412; 7.409; 7.406; 7.403; 7.400; 7.397; 7.394; 7.390; 7.387; 7.384; 7.381; 7.378; 7.375; 7.372; 7.369; 7.366; 7.363; 7.360; 7.357; 7.353; 7.350; 7.347; 7.344; 7.341; 7.338; 7.335; 7.332; 7.329; 7.326; 7.323; 7.320; 7.317; 7.314; 7.311; 7.308; 7.305; 7.302; 7.300; 7.297; 7.294; 7.291; 7.288; 7.285; 7.282; 7.279; 7.276; 7.273; 7.270; 7.267; 7.264; 7.262; 7.259; 7.256; 7.253; 7.250; 7.247; 7.244; 7.241; 7.239; 7.236; 7.233; 7.230; 7.227; 7.224; 7.221; 7.219; 7.216; 7.213; 7.210; 7.207; 7.204; 7.202; 7.199; 7.196; 7.193; 7.190; 7.188; 7.185; 7.182; 7.179; 7.177; 7.174; 7.171; 7.168; 7.165; 7.163; 7.160; 7.157; 7.154; 7.152; 7.149; 7.146; 7.144; 7.141; 7.138; 7.135; 7.133; 7.130; 7.127; 7.125; 7.122; 7.119; 7.116; 7.114; 7.111; 7.108; 7.106; 7.103; 7.100; 7.098; 7.095; 7.092; 7.090; 7.087; 7.084; 7.082; 7.079; 7.076; 7.074; 7.071; 7.069; 7.066; 7.063; 7.061; 7.058; 7.055; 7.053; 7.050; 7.048; 7.045; 7.042; 7.040; 7.037; 7.035; 7.032; 7.030; 7.027; 7.024; 7.022; 7.019; 7.017; 7.014; 7.012; 7.009; 7.006; 7.004; 7.001; 6.999; 6.996; 6.994; 6.991; 6.989; 6.986; 6.984; 6.981; 6.979; 6.976; 6.974; 6.971; 6.969; 6.966; 6.964; 6.961; 6.959; 6.956; 6.954; 6.951; 6.949; 6.946; 6.944; 6.941; 6.939; 6.936; 6.934; 6.931; 6.929; 6.926; 6.924; 6.921; 6.919; 6.917; 6.914; 6.912; 6.909; 6.907; 6.904; 6.902; 6.900; 6.897; 6.895; 6.892; 6.890; 6.887; 6.885; 6.883; 6.880; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(epochs = 850, lr = 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring RMSLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_detached = [c.detach() for c in coeffs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 47.8076],\n",
       "        [ 22.3647],\n",
       "        [ 15.1160],\n",
       "        [ 62.8945],\n",
       "        [145.9174],\n",
       "        [ 73.9797],\n",
       "        [ 91.7850],\n",
       "        [103.2430],\n",
       "        [183.2233],\n",
       "        [ 43.7155],\n",
       "        [ 96.8142],\n",
       "        [131.8031],\n",
       "        [192.7808],\n",
       "        [ 69.4947],\n",
       "        [ 81.3401],\n",
       "        [ 94.7430],\n",
       "        [ 61.4321],\n",
       "        [ 76.5971],\n",
       "        [ 28.1045],\n",
       "        [  3.7485]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_lr_re = calc_preds(coeffs_detached, val_indep)\n",
    "res_lr_re[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(pred, val_dep):\n",
    "    rmsle = torch.abs((torch.log(1+pred) - torch.log(1+val_dep)).mean())\n",
    "    return rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0117, dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(res_lr_re, val_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.get_dummies(test_df, columns=[\"Sex\"], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_indep = tensor(test_df.values, dtype=torch.float)\n",
    "tst_indep = tst_indep/vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = calc_preds(coeffs_detached, tst_indep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Calories'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test[['id','Calories']]\n",
    "sub_df.to_csv('sub_relu_bias.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,Calories\n",
      "750000,29.786493\n",
      "750001,110.42535\n",
      "750002,82.10322\n",
      "750003,116.03846\n",
      "750004,83.3504\n",
      "750005,23.146364\n",
      "750006,44.3384\n",
      "750007,7.399134\n",
      "750008,9.549758\n"
     ]
    }
   ],
   "source": [
    "!head sub_relu.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net in the previous section only uses one hidden layer, so it doesn't count as \"deep\" learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\n",
    "\n",
    "First, we'll need to create additional coefficients for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coeffs():\n",
    "    hiddens = [40, 20]  # <-- set this to the size of each hidden layer you want\n",
    "    sizes = [n_coeff] + hiddens + [1]\n",
    "    n = len(sizes)\n",
    "    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n",
    "    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n",
    "    for l in layers+consts: l.requires_grad_()\n",
    "    return layers,consts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building and training the deep learning model, it as observed that even small tweaks to the initialization of the weights made a big difference — sometimes causing the model to completely fail to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calc_preds(coeffs, indeps):\n",
    "    layers,consts = coeffs\n",
    "    n = len(layers)\n",
    "    res = indeps\n",
    "    for i,l in enumerate(layers):\n",
    "        res = res@l + consts[i]\n",
    "        if i!=(n-1): \n",
    "            res = F.leaky_relu(res, negative_slope=0.01)\n",
    "        else:\n",
    "            res = F.relu(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a minor update to update_coeffs since we've got layers and consts separated now:`m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeffs(coeffs, lr):\n",
    "    layers,consts = coeffs\n",
    "    for layer in layers+consts:\n",
    "        layer.sub_(layer.grad * lr)\n",
    "        layer.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=30, lr=0.01):\n",
    "    torch.manual_seed(442)\n",
    "    coeffs = init_coeffs()\n",
    "    for i in range(epochs): \n",
    "        res = calc_preds(coeffs, trn_indep)\n",
    "        loss = torch.abs((res) - trn_dep).mean()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            update_coeffs(coeffs, lr)\n",
    "        print(f\"{loss:.3f}\", end=\"; \")\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.915; 84.740; 82.364; 79.981; 77.698; 75.469; 73.307; 71.193; 69.146; 67.187; 65.319; 63.566; 61.929; 60.410; 59.021; 57.767; 56.684; 55.740; 54.938; 54.261; 53.708; 53.254; 52.882; 52.591; 52.359; 52.175; 52.031; 51.916; 51.825; 51.754; 51.698; 51.653; 51.616; 51.586; 51.560; 51.538; 51.519; 51.502; 51.486; 51.472; 51.458; 51.445; 51.433; 51.421; 51.409; 51.398; 51.386; 51.375; 51.364; 51.353; 51.342; 51.330; 51.319; 51.308; 51.297; 51.286; 51.275; 51.264; 51.253; 51.242; 51.231; 51.220; 51.208; 51.197; 51.186; 51.175; 51.163; 51.152; 51.141; 51.129; 51.118; 51.107; 51.095; 51.084; 51.072; 51.061; 51.049; 51.038; 51.026; 51.015; 51.003; 50.991; 50.980; 50.968; 50.956; 50.944; 50.932; 50.921; 50.909; 50.897; 50.885; 50.873; 50.861; 50.849; 50.837; 50.824; 50.812; 50.800; 50.788; 50.775; 50.763; 50.751; 50.738; 50.726; 50.713; 50.701; 50.688; 50.676; 50.663; 50.650; 50.638; 50.625; 50.612; 50.599; 50.586; 50.573; 50.560; 50.547; 50.534; 50.521; 50.508; 50.495; 50.482; 50.468; 50.455; 50.442; 50.428; 50.415; 50.401; 50.387; 50.374; 50.360; 50.346; 50.332; 50.319; 50.305; 50.291; 50.277; 50.263; 50.248; 50.234; 50.220; 50.206; 50.191; 50.177; 50.162; 50.148; 50.133; 50.118; 50.104; 50.089; 50.074; 50.059; 50.044; 50.029; 50.014; 49.999; 49.983; 49.968; 49.953; 49.937; 49.922; 49.906; 49.890; 49.875; 49.859; 49.843; 49.827; 49.811; 49.795; 49.779; 49.762; 49.746; 49.730; 49.713; 49.696; 49.680; 49.663; 49.646; 49.629; 49.612; 49.595; 49.578; 49.561; 49.543; 49.526; 49.508; 49.491; 49.473; 49.455; 49.437; 49.419; 49.401; 49.383; 49.365; 49.347; 49.328; 49.310; 49.291; 49.272; 49.253; 49.234; 49.215; 49.196; 49.177; 49.157; 49.138; 49.118; 49.099; 49.079; 49.059; 49.039; 49.019; 48.999; 48.978; 48.958; 48.937; 48.916; 48.896; 48.875; 48.854; 48.832; 48.811; 48.790; 48.768; 48.746; 48.725; 48.703; 48.681; 48.658; 48.636; 48.614; 48.591; 48.568; 48.545; 48.522; 48.499; 48.476; 48.452; 48.429; 48.405; 48.381; 48.357; 48.333; 48.309; 48.284; 48.260; 48.235; 48.210; 48.185; 48.160; 48.134; 48.109; 48.083; 48.057; 48.031; 48.005; 47.978; 47.952; 47.925; 47.898; 47.871; 47.844; 47.816; 47.789; 47.761; 47.733; 47.705; 47.676; 47.648; 47.619; 47.590; 47.561; 47.532; 47.502; 47.473; 47.443; 47.413; 47.382; 47.352; 47.321; 47.290; 47.259; 47.228; 47.196; 47.165; 47.133; 47.101; 47.068; 47.035; 47.003; 46.970; 46.936; 46.903; 46.869; 46.835; 46.801; 46.766; 46.731; 46.697; 46.661; 46.626; 46.590; 46.554; 46.518; 46.481; 46.445; 46.408; 46.370; 46.333; 46.295; 46.257; 46.219; 46.180; 46.141; 46.102; 46.063; 46.023; 45.983; 45.943; 45.902; 45.861; 45.820; 45.778; 45.737; 45.695; 45.652; 45.609; 45.566; 45.523; 45.479; 45.435; 45.391; 45.346; 45.301; 45.256; 45.211; 45.165; 45.118; 45.072; 45.025; 44.977; 44.929; 44.881; 44.833; 44.784; 44.735; 44.685; 44.636; 44.585; 44.535; 44.484; 44.432; 44.380; 44.328; 44.276; 44.223; 44.169; 44.116; 44.062; 44.008; 43.953; 43.898; 43.843; 43.787; 43.731; 43.674; 43.617; 43.560; 43.502; 43.444; 43.385; 43.326; 43.266; 43.206; 43.145; 43.084; 43.022; 42.960; 42.898; 42.835; 42.771; 42.707; 42.643; 42.578; 42.512; 42.446; 42.380; 42.313; 42.245; 42.177; 42.109; 42.040; 41.970; 41.900; 41.829; 41.758; 41.686; 41.613; 41.540; 41.466; 41.392; 41.317; 41.242; 41.166; 41.089; 41.012; 40.934; 40.856; 40.777; 40.697; 40.617; 40.536; 40.454; 40.372; 40.289; 40.205; 40.121; 40.036; 39.951; 39.864; 39.777; 39.689; 39.601; 39.512; 39.422; 39.331; 39.240; 39.148; 39.055; 38.961; 38.867; 38.772; 38.676; 38.579; 38.482; 38.383; 38.284; 38.184; 38.083; 37.982; 37.879; 37.776; 37.672; 37.567; 37.461; 37.354; 37.246; 37.138; 37.028; 36.918; 36.807; 36.694; 36.581; 36.467; 36.352; 36.236; 36.119; 36.001; 35.882; 35.762; 35.641; 35.519; 35.396; 35.272; 35.147; 35.020; 34.893; 34.765; 34.636; 34.506; 34.375; 34.243; 34.110; 33.976; 33.841; 33.704; 33.567; 33.428; 33.288; 33.147; 33.004; 32.860; 32.716; 32.570; 32.422; 32.274; 32.124; 31.973; 31.820; 31.666; 31.511; 31.355; 31.197; 31.038; 30.878; 30.716; 30.553; 30.389; 30.223; 30.056; 29.887; 29.717; 29.546; 29.373; 29.199; 29.023; 28.846; 28.668; 28.488; 28.307; 28.124; 27.940; 27.754; 27.567; 27.379; 27.189; 26.998; 26.805; 26.611; 26.415; 26.218; 26.019; 25.819; 25.618; 25.415; 25.210; 25.004; 24.797; 24.588; 24.378; 24.167; 23.955; 23.741; 23.526; 23.310; 23.093; 22.875; 22.656; 22.436; 22.216; 21.995; 21.773; 21.550; 21.326; 21.102; 20.877; 20.653; 20.430; 20.214; "
     ]
    }
   ],
   "source": [
    "coeffs = train_model(epochs = 550, lr = 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_trained = coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with lower learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.058; 6.920; 6.879; 6.821; 6.800; 6.768; 6.756; 6.737; 6.729; 6.718; 6.712; 6.705; 6.702; 6.697; 6.695; 6.692; 6.690; 6.688; 6.686; 6.685; 6.684; 6.683; 6.682; 6.681; 6.680; 6.680; 6.679; 6.678; 6.678; 6.677; 6.677; 6.676; 6.676; 6.675; 6.675; 6.674; 6.674; 6.674; 6.673; 6.673; 6.672; 6.672; 6.671; 6.671; 6.670; 6.670; 6.669; 6.669; 6.668; 6.668; 6.667; 6.667; 6.667; 6.666; 6.666; 6.665; 6.665; 6.664; 6.664; 6.663; 6.663; 6.662; 6.662; 6.661; 6.661; 6.661; 6.660; 6.660; 6.659; 6.659; 6.658; 6.658; 6.657; 6.657; 6.656; 6.656; 6.656; 6.655; 6.655; 6.654; 6.654; 6.653; 6.653; 6.652; 6.652; 6.651; 6.651; 6.650; 6.650; 6.650; 6.649; 6.649; 6.648; 6.648; 6.647; 6.647; 6.646; 6.646; 6.645; 6.645; 6.645; 6.644; 6.644; 6.643; 6.643; 6.642; 6.642; 6.641; 6.641; 6.640; 6.640; 6.640; 6.639; 6.639; 6.638; 6.638; 6.637; 6.637; 6.636; 6.636; 6.636; 6.635; 6.635; 6.634; 6.634; 6.633; 6.633; 6.632; 6.632; 6.631; 6.631; 6.631; 6.630; 6.630; 6.629; 6.629; 6.628; 6.628; 6.627; 6.627; 6.627; 6.626; 6.626; 6.625; 6.625; 6.624; 6.624; 6.623; 6.623; 6.622; 6.622; 6.622; 6.621; 6.621; 6.620; 6.620; 6.619; 6.619; 6.618; 6.618; 6.618; 6.617; 6.617; 6.616; 6.616; 6.615; 6.615; 6.614; 6.614; 6.614; 6.613; 6.613; 6.612; 6.612; 6.611; 6.611; 6.611; 6.610; 6.610; 6.609; 6.609; 6.608; 6.608; 6.607; 6.607; 6.607; 6.606; 6.606; 6.605; 6.605; 6.604; 6.604; 6.603; 6.603; 6.603; 6.602; 6.602; 6.601; 6.601; 6.600; 6.600; 6.600; 6.599; 6.599; 6.598; 6.598; 6.597; 6.597; 6.596; 6.596; 6.596; 6.595; 6.595; 6.594; 6.594; 6.593; 6.593; 6.593; 6.592; 6.592; 6.591; 6.591; 6.590; 6.590; 6.589; 6.589; 6.589; 6.588; 6.588; 6.587; 6.587; 6.586; 6.586; 6.586; 6.585; 6.585; 6.584; 6.584; 6.583; 6.583; 6.583; 6.582; 6.582; 6.581; 6.581; 6.580; 6.580; 6.580; 6.579; 6.579; 6.578; 6.578; 6.577; 6.577; 6.577; 6.576; 6.576; 6.575; 6.575; 6.574; 6.574; 6.574; 6.573; 6.573; 6.572; 6.572; 6.571; 6.571; 6.571; 6.570; 6.570; 6.569; 6.569; 6.568; 6.568; 6.568; 6.567; 6.567; 6.566; 6.566; 6.565; 6.565; 6.565; 6.564; 6.564; 6.563; 6.563; 6.562; 6.562; 6.562; 6.561; 6.561; 6.560; 6.560; 6.559; 6.559; 6.559; 6.558; 6.558; 6.557; 6.557; 6.556; 6.556; 6.556; 6.555; 6.555; 6.554; 6.554; 6.554; 6.553; 6.553; 6.552; 6.552; 6.551; 6.551; 6.551; 6.550; 6.550; 6.549; 6.549; 6.548; 6.548; 6.548; 6.547; 6.547; 6.546; 6.546; 6.545; 6.545; 6.545; 6.544; 6.544; 6.543; 6.543; 6.543; 6.542; 6.542; 6.541; 6.541; 6.540; 6.540; 6.540; 6.539; 6.539; 6.538; 6.538; 6.538; 6.537; 6.537; 6.536; 6.536; 6.535; 6.535; 6.535; 6.534; 6.534; 6.533; 6.533; 6.532; 6.532; 6.532; 6.531; 6.531; 6.530; 6.530; 6.530; 6.529; 6.529; 6.528; 6.528; 6.527; 6.527; 6.527; 6.526; 6.526; 6.525; 6.525; 6.525; 6.524; 6.524; 6.523; 6.523; 6.522; 6.522; 6.522; 6.521; 6.521; 6.520; 6.520; 6.520; 6.519; 6.519; 6.518; 6.518; 6.517; 6.517; 6.517; 6.516; 6.516; 6.515; 6.515; 6.515; 6.514; 6.514; 6.513; 6.513; 6.513; 6.512; 6.512; 6.511; 6.511; 6.510; 6.510; 6.510; 6.509; 6.509; 6.508; 6.508; 6.508; 6.507; 6.507; 6.506; 6.506; 6.506; 6.505; 6.505; 6.504; 6.504; 6.503; 6.503; 6.503; 6.502; 6.502; 6.501; 6.501; 6.501; 6.500; 6.500; 6.499; 6.499; 6.499; 6.498; 6.498; 6.497; 6.497; 6.496; 6.496; 6.496; 6.495; 6.495; 6.494; 6.494; 6.494; 6.493; 6.493; 6.492; 6.492; 6.492; 6.491; 6.491; 6.490; 6.490; 6.490; 6.489; 6.489; 6.488; 6.488; 6.487; 6.487; 6.487; 6.486; 6.486; 6.485; 6.485; 6.485; 6.484; 6.484; 6.483; 6.483; 6.483; 6.482; 6.482; 6.481; 6.481; 6.481; 6.480; 6.480; 6.479; 6.479; 6.479; 6.478; 6.478; 6.477; 6.477; 6.477; 6.476; 6.476; 6.475; 6.475; 6.475; "
     ]
    }
   ],
   "source": [
    "for i in range(500): \n",
    "    res = calc_preds(coeffs_trained, trn_indep)\n",
    "    loss = torch.abs((res) - trn_dep).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        update_coeffs(coeffs_trained, 0.00035)\n",
    "    print(f\"{loss:.3f}\", end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_prev = coeffs_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_trained = coeffs_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48.8859],\n",
       "        [ 22.5483],\n",
       "        [ 16.1244],\n",
       "        [ 62.2050],\n",
       "        [143.6851],\n",
       "        [ 74.4506],\n",
       "        [ 90.9737],\n",
       "        [101.5608],\n",
       "        [180.7012],\n",
       "        [ 45.0365],\n",
       "        [ 96.8265],\n",
       "        [134.1979],\n",
       "        [192.6899],\n",
       "        [ 70.7964],\n",
       "        [ 77.4568],\n",
       "        [ 89.8667],\n",
       "        [ 59.8620],\n",
       "        [ 82.0239],\n",
       "        [ 27.4504],\n",
       "        [  4.3156]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dnn = calc_preds(coeffs_prev, val_indep)\n",
    "res_dnn[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0141, dtype=torch.float64, grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmsle(res_dnn, val_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = calc_preds(coeffs_prev, tst_indep)\n",
    "test['Calories'] = preds_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test[['id','Calories']]\n",
    "sub_df.to_csv('sub_dnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,Calories\n",
      "750000,27.496075\n",
      "750001,110.738235\n",
      "750002,82.47298\n",
      "750003,116.671295\n",
      "750004,77.61338\n",
      "750005,22.979803\n",
      "750006,46.875248\n",
      "750007,7.8778753\n",
      "750008,9.256768\n"
     ]
    }
   ],
   "source": [
    "!head sub_dnn.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11893428,
     "sourceId": 91716,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
